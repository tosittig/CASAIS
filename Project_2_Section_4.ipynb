{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 2 Section 4: Object Detection and Image Segmentation\n",
        "\n",
        "We learn here how to detect objects in images and how to segment an image according to the objects in it.\n",
        "\n",
        "## Tutorial 1: Object detection\n",
        "\n",
        "Object Detection helps us understand the spatial context and location of different objects in an image. Below, you are going to perform Object detection with the use of the pretrained DETR model.\n",
        "\n",
        "The DETR model is a complex CNN that takes as input an image and outputs the same image annotated with boxes identifying the objects in it. It was trained on COCO 2017, a dataset of 118k annotated images.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KCbtisSPs2mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information about the DETR model, please see [End-to-End Object Detection with Transformers](https://https://arxiv.org/abs/2005.12872)\n",
        "\n",
        "To start, you need to install the transformers and timm package."
      ],
      "metadata": {
        "id": "reZbcOJYu9Hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtT8fJYZW2iG"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the transformers module import the classes `DetrImageProcessor` and `DetrForObjectDetection`."
      ],
      "metadata": {
        "id": "rHecQXVivIXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DetrImageProcessor, DetrForObjectDetection"
      ],
      "metadata": {
        "id": "0QFmSzvkvJfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `DetrImageProcessor` class is used for the pre-processing of the images which then will be used as input to the DETR model.\n",
        "* The `DetrForObjectDetection` class provides access to the pre-trained DETR model."
      ],
      "metadata": {
        "id": "LcgRNeO7vMr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model `facebook/detr-resnet-50` for preprocessing.\n"
      ],
      "metadata": {
        "id": "KJpnze2nvRYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")"
      ],
      "metadata": {
        "id": "4ye6awS0vRDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the object detector model `facebook/detr-resnet-50`."
      ],
      "metadata": {
        "id": "oPU1fNmevb2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
      ],
      "metadata": {
        "id": "8Q8nFdCfvejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the 89 different objects that the model has been trained to recognize, with the `.config.id2label` attribute."
      ],
      "metadata": {
        "id": "18N7Yt1Lvitd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "KocuiIrkXOns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, it can recognise objects like: a bird, a cat, a hat, a car, a tie, a bicycle etc."
      ],
      "metadata": {
        "id": "MK1Z7Gz-voxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now load the following image from the Internet."
      ],
      "metadata": {
        "id": "T26t2AsTvtPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "A5-RGDdPXPgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the image."
      ],
      "metadata": {
        "id": "-hkywaULv-t7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = image_processor(images = image,\n",
        "                         return_tensors = \"pt\")\n",
        "outputs = model(**inputs)\n",
        "target_sizes = torch.tensor([image.size[::-1]])"
      ],
      "metadata": {
        "id": "J4ORGm9QZjKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the `post_process_object_detection()` method, return a dictionary which will contain the objects detected in the image. The three key-value pairs in the dictionary are:\n",
        "\n",
        "* scores — The confidence of each detected object\n",
        "* labels — the index of the detected object in model.config.id2label\n",
        "* boxes — the bounding boxes of each detected object\n",
        "\n",
        "The `post_process_object_detection()` method takes in the following arguments:\n",
        "\n",
        "* the output of the model (outputs)\n",
        "* the target size of the image (target_sizes)\n",
        "* the threshold value (0.9) for filtering out predictions, which means that predictions with confidence greater than 90% will be returned.\n"
      ],
      "metadata": {
        "id": "VGgARFLzwYMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = image_processor.post_process_object_detection(outputs,\n",
        "                                                        target_sizes = target_sizes,\n",
        "                                                        threshold = 0.9)[0]\n",
        "results"
      ],
      "metadata": {
        "id": "M__X-C2LZ0Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise the detected objects with their confidence scores, labels and drawing boxes around them."
      ],
      "metadata": {
        "id": "-YwUh1tGwiZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "        f\"{round(score.item(), 3)} at location {box}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "m60_owMjiQaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw bounding boxes around objects"
      ],
      "metadata": {
        "id": "NPCgMiVI05dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw.rectangle(box, outline=\"yellow\", width=2)"
      ],
      "metadata": {
        "id": "g51NXkNs08sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the labels"
      ],
      "metadata": {
        "id": "kesYqDy01Ds9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw.text((box[0], box[1]-10),\n",
        "          model.config.id2label[label.item()],\n",
        "          fill=\"white\")\n",
        "image"
      ],
      "metadata": {
        "id": "J7SDdQ9t1CS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial 2 (optional)\n",
        "\n",
        "Image segmentation is the task of segmenting an image into multiple regions, each of which contains a set of pixels. There are various image segmentation techniques. In this tutorial we are going to focus on the simple thresholding technique with the use of the OpenCV library.\n",
        "\n",
        "### Simple thresholding\n",
        "\n",
        "Simple thresholding involves setting a threshold value for the entire image. Pixels with intensity values which are above the threshold value are segmented as foreground, while pixels with intensity values which are below the threshold value are segmented as background. Binarization is the outcome of thresholding, resulting in an image of only two colours: black and white.\n"
      ],
      "metadata": {
        "id": "_OyV7flQ1OZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import urllib.request\n",
        "\n",
        "url = 'https://carwow-uk-wp-3.imgix.net/18015-MC20BluInfinito-scaled-e1666008987698.jpg'\n",
        "resp = urllib.request.urlopen(url)\n",
        "image_array = np.asarray(bytearray(resp.read()), dtype=np.uint8)\n",
        "image = cv2.imdecode(image_array, -1)"
      ],
      "metadata": {
        "id": "PoSCcVHz2LYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the image as grayscale with the `cv2.cvtColor( )` method."
      ],
      "metadata": {
        "id": "jnriV2RC3Ahn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
      ],
      "metadata": {
        "id": "szwOkJC39iVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply simple thresholding with the `cv2.threshold( )` method which takes the following parameters (in this order):\n",
        "\n",
        "* `source`: the input image which is grayscale\n",
        "* `thresholdValue`: if the pixel value is smaller than the threshold (128), it is set to 0, otherwise, it is set to a maximum value (255)\n",
        "* `maxVal`: maximum value that can be assigned to a pixel; the threshold type has been set to `THRESH_BINARY` which means that if the pixel intensity value is above the threshold value is set to the maximum value 255, and if it is below the threshold value, it is set to 0."
      ],
      "metadata": {
        "id": "GTsiWQW63HY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, thresholded_image = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(thresholded_image)"
      ],
      "metadata": {
        "id": "DDynbz-5zhmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* the threshold type has been set to `THRESH_TRUNC` which means that pixel intensity values which are lower than the threshold value remain unchanged, but pixel intensity values which are higher than the threshold value are set to the threshold value of 128\n"
      ],
      "metadata": {
        "id": "y2EvPlg13Oor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, thresholded_image = cv2.threshold(img, 128, 255, cv2.THRESH_TRUNC)\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(thresholded_image)"
      ],
      "metadata": {
        "id": "urr62UjYDXjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* the threshold type has been set to `THRESH_TOZERO` which means that pixel intensity values which are higher than the threshold value remain unchanged and pixel intensity values which are lower than the threshold value are set to zero\n"
      ],
      "metadata": {
        "id": "PZ8IVJ1d3Wt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, thresholded_image = cv2.threshold(img, 128, 255, cv2.THRESH_TOZERO)\n",
        "cv2_imshow(image)\n",
        "cv2_imshow(thresholded_image)"
      ],
      "metadata": {
        "id": "w5_dRA2tDea4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial 3 (optional, semantic image segmentation)\n",
        "\n",
        "We start by importing necessary tools.\n",
        " - torch: A powerful library for working with artificial intelligence models.\n",
        " - torchvision: Offers tools and models specifically for image tasks.\n",
        " - PIL (Python Imaging Library): Helps us work with images (loading, displaying).\n",
        " - requests: Enables us to fetch data from the internet.\n",
        " - matplotlib: A plotting library, for showing images and results."
      ],
      "metadata": {
        "id": "U_f77wqhRNE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.segmentation import deeplabv3_resnet101\n",
        "from PIL import Image\n",
        "import requests\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "SJZwwRjGRtKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the model\n",
        "\n",
        "We now load a pre-trained CNN, DeepLabV3, which has been trained to recognize various objects in images. The model is set to \"evaluation mode,\" indicating it's ready to analyze images."
      ],
      "metadata": {
        "id": "sBGCHR5eR4Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = deeplabv3_resnet101(pretrained=True)\n",
        "model.eval()  # We tell the model it's time to work (evaluation mode)."
      ],
      "metadata": {
        "id": "Aa9yXXpmSF_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the image\n",
        "\n",
        "We retrieve an image from the internet to analyze. The image is opened and ready to be processed, similar to selecting a photograph to examine."
      ],
      "metadata": {
        "id": "JmMaY5vgSMLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = 'https://cdn.pixabay.com/photo/2018/10/01/09/21/pets-3715733_960_720.jpg'\n",
        "response = requests.get(image_url)\n",
        "# Open the image using PIL, a library that understands image formats.\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)"
      ],
      "metadata": {
        "id": "3DWN2ZPsSLtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the image\n",
        "\n",
        "We now transform the image into a format suitable for the model. It's akin to translating a document into a language the expert (our CNN) understands. This step ensures the image is correctly interpreted by the CNN."
      ],
      "metadata": {
        "id": "ydJ-Nvl9SS8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Before analyzing, we need to adjust the image to the format the model expects.\n",
        "# This involves converting the image to a tensor (a multi-dimensional array used in AI models) and normalizing it.\n",
        "preprocess = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # The model expects a batch of images, even if there's just one."
      ],
      "metadata": {
        "id": "QdbkmqdKSfQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analyzing the image\n",
        "\n",
        "Here, the model examines the image and determines which parts of the image belong to different objects. This process is similar to asking an expert to identify and categorize different elements in a photograph.\n"
      ],
      "metadata": {
        "id": "FaDaWiuXTP1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we ask our pre-trained model to analyze the image and give us the segmentation result.\n",
        "with torch.no_grad():  # This tells PyTorch we don't need to do any training.\n",
        "    output = model(input_batch)['out'][0]\n",
        "output_predictions = output.argmax(0)"
      ],
      "metadata": {
        "id": "uV_br7sETlVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizing the results\n",
        "\n",
        "This final step displays the original image alongside the segmentation result produced by our model. The segmentation map uses different colors to represent various parts of the image identified by the model, offering a visual representation of the model's \"understanding\" of the scene."
      ],
      "metadata": {
        "id": "B9392YfDTnD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we visualize the results. We'll show the original image and the model's understanding of it side by side.\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')  # Hide the axis for a cleaner look\n",
        "plt.subplot(1, 2, 2)\n",
        "# We use a color map to differentiate the segments identified by the model.\n",
        "plt.imshow(output_predictions.byte().cpu().numpy(), cmap='nipy_spectral', interpolation='nearest')\n",
        "plt.title('Segmentation Result')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WZ2irsOXTspu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe how the CNN recognizes and distinguishes between cats and dogs. The cats are colored in green and the dogs are colored in gray. Note however, that it incorrectly believes that there is a cat in front of the two dogs."
      ],
      "metadata": {
        "id": "eggXLZceTy_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessment\n",
        "\n",
        "You must choose either Task 1 or Task 2.\n",
        "\n",
        "### Task 1\n",
        "\n",
        "Perform object detection on this image [cat and dog](https://www.companionanimalclinicvirginia.com/wp-content/uploads/2018/12/white_cat_and_dog.jpg) by using the DETR pre-trained model. Here is the URL `https://www.companionanimalclinicvirginia.com/wp-content/uploads/2018/12/white_cat_and_dog.jpg`.\n",
        "\n",
        "### Task 2\n",
        "\n",
        "Perform image segmentation on this image [house](https://www.bhg.com/thmb/H9VV9JNnKl-H1faFXnPlQfNprYw=/1799x0/filters:no_upscale():strip_icc()/white-modern-house-curved-patio-archway-c0a4a3b3-aa51b24d14d0464ea15d36e05aa85ac9.jpg) with the use of the thresholding technique in the OpenCV library. You can apply either `THRESH_BINARY` or `THRESH_TRUNC` or `THRESH_TOZERO` technique. Here is the URL `https://www.bhg.com/thmb/H9VV9JNnKl-H1faFXnPlQfNprYw=/1799x0/filters:no_upscale():strip_icc()/white-modern-house-curved-patio-archway-c0a4a3b3-aa51b24d14d0464ea15d36e05aa85ac9.jpg`.\n",
        "\n",
        "### Task 3\n",
        "\n",
        "Perform semantic image segmentation on this [image](https://cdn.pixabay.com/photo/2017/12/27/14/02/friends-3042751_960_720.jpg). Here is the URL `https://cdn.pixabay.com/photo/2017/12/27/14/02/friends-3042751_960_720.jpg`. Use the following models:\n",
        "\n",
        "* FCN: You can load this model with the command `model = fcn_resnet101(pretrained=True)`. Do not forget to import it.\n",
        "* LRASPP MobileNetV3: You can load this model with the command `model = lraspp_mobilenet_v3_large(pretrained=True)`.\n",
        "\n",
        "Which one works best for this image? Do the same for the image of the cats and dogs. Which one works best for that image?"
      ],
      "metadata": {
        "id": "3Ovvwv5r3rCR"
      }
    }
  ]
}