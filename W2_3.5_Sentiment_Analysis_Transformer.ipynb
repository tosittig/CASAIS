{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qLvl1v98A4_"
   },
   "source": [
    "# Sentiment Classification With a Transformer\n",
    "\n",
    "In this notebook, we revert to the classification task of the International Movie Database website www.imdb.com with reviews labeled with a binary rating whether they are positive (label 1) or negative (label 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8DKB6tD88y-"
   },
   "source": [
    "## Set-up\n",
    "First of all, we need to load the libraries that we will need for this task. We will use keras and tensorflow for this code example, so we load the relevant parts of this framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4509,
     "status": "ok",
     "timestamp": 1702289362631,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "r2mlKlDL9Sqx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1702289362632,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "Lmpc4ffC9tuz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Input, TextVectorization, Embedding, Conv1D, MaxPooling1D, Flatten, LSTM, Bidirectional\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1702289362633,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "bkv-ED06a5FA"
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "tf.experimental.numpy.experimental_enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1702289362633,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "yjdwz9uU9vZM"
   },
   "outputs": [],
   "source": [
    "# initialize random number generators to ensure reproducibility:\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1702289362633,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "k9CKWApuMOFW"
   },
   "outputs": [],
   "source": [
    "# some more general libraries for evaluation purposes:\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16985,
     "status": "ok",
     "timestamp": 1702289379614,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "3Zu9jRSjcxkk",
    "outputId": "00a3a0de-4901-477b-b683-edbfb0dc376d"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "no13ZU2gfXbN"
   },
   "source": [
    "Define parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1702289380019,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "rgz2s6zX7RPH"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000  # Only consider the top 20k words\n",
    "MAX_LEN = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "EMBED_DIM = 100  # Embedding size for each token\n",
    "NUM_HEADS = 3  # Number of attention heads\n",
    "FF_DIM = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Google drive (if used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1702289380019,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "_Al4WWatKYe2"
   },
   "outputs": [],
   "source": [
    "use_gdrive = False\n",
    "\n",
    "if use_gdrive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    targetDir_root = 'gdrive/MyDrive/CAS_AIS_2024_FS/Results/'\n",
    "else:\n",
    "    targetDir_root = './'\n",
    "    \n",
    "targetDir_models = targetDir_root + 'trainedWeights/'\n",
    "targetDir_results = targetDir_root + 'PerformanceMeasures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpDbXtQ1fdpi"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "We now use a different, more direct way to get the imdb data. Using the tensorflow keras datasets subpackage, we can directly get a vectorized representation of the imdb movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4221,
     "status": "ok",
     "timestamp": 1702290225052,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "ord49V1FLBfg",
    "outputId": "9c1c03ce-a117-42f7-88a3-1f477edab0fe"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=VOCAB_SIZE)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = tf.keras.utils.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_val = tf.keras.utils.pad_sequences(x_val, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8orCMnV8KGXm"
   },
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1702290230116,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "YY6sh4X3MLOs"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1702290231260,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "6XHdRdFqLdr7"
   },
   "outputs": [],
   "source": [
    "if use_gdrive:\n",
    "    %run gdrive/MyDrive/CAS_AIS_2024_FS/Colab_Notebooks/W2_3.4_Transformers.ipynb\n",
    "else:\n",
    "    %run W2_3.4_Transformers.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_FQaAf8gXaA"
   },
   "source": [
    "# Review Rater with Transformers\n",
    "Using the building blocks defined in the previous notebook, we can simply combine the necessary blocks. Here, we use the Encoder part of the transformer, and then just add a few layers as classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1702290233062,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "qWu7SyZ1Qrwi"
   },
   "outputs": [],
   "source": [
    "# initialize random number generators to ensure reproducibility:\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1702290385537,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "XEHE7e9HJ0ic"
   },
   "outputs": [],
   "source": [
    "class ReviewRater(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, embed_dim, num_heads, ff_dim,\n",
    "                 input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # Encoder Block\n",
    "        self.encoder = Encoder(num_layers=num_layers, embed_dim=embed_dim,\n",
    "                               num_heads=num_heads, ff_dim=ff_dim,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        # custom classification head\n",
    "        self.globAvgPool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.dense20 = tf.keras.layers.Dense(20, activation=\"relu\")\n",
    "        self.dense2  = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.globAvgPool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense20(x)\n",
    "        outputs = self.dense2(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702290385804,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "tmvHbZf9E22S"
   },
   "outputs": [],
   "source": [
    "model_5kW_trans_RR = ReviewRater(num_layers=1, embed_dim=EMBED_DIM, num_heads=NUM_HEADS,\n",
    "                                 ff_dim=FF_DIM, input_vocab_size=VOCAB_SIZE,\n",
    "                                 target_vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5kW_trans_RR.compile(loss = BinaryCrossentropy(from_logits=False),\n",
    "                           optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the previous models, we only train the models from scratch if needed, and load the pre-trained model weights and results from files otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scatch = True\n",
    "\n",
    "model_name = 'model_5kW_trans_RR'\n",
    "model_weight_file = model_name + '_weights'\n",
    "model_result_file = model_name + '_Results.pkl'\n",
    "\n",
    "if train_from_scatch: \n",
    "    myRRHistory = model_5kW_trans_RR.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data = (x_val, y_val),\n",
    "        epochs = NUM_EPOCHS, verbose = 1,\n",
    "        callbacks = [ EarlyStopping(monitor='val_accuracy', patience=5,\n",
    "                                    verbose=False, restore_best_weights=True)])\n",
    "\n",
    "    myRRHistory_dict = myRRHistory.history\n",
    "    resDict_5kW_trans_RR = {}\n",
    "    resDict_5kW_trans_RR['train_loss'] = myRRHistory_dict['loss']\n",
    "    resDict_5kW_trans_RR['val_loss'] = myRRHistory_dict['val_loss']\n",
    "    resDict_5kW_trans_RR['train_accuracy'] = myRRHistory_dict['accuracy']\n",
    "    resDict_5kW_trans_RR['val_accuracy'] = myRRHistory_dict['val_accuracy']\n",
    "    resDict_5kW_trans_RR['epochs'] = range(1, len(resDict_5kW_trans_RR['train_accuracy']) + 1)\n",
    "    resDict_5kW_trans_RR['model_name'] = model_name\n",
    "    \n",
    "    # save weights and results\n",
    "    model_5kW_trans_RR.save_weights(model_weight_file)\n",
    "    with open(model_result_file, 'wb') as f:\n",
    "        pickle.dump(resDict_5kW_trans_RR, f)\n",
    "else:\n",
    "    model_5kW_trans_RR.load_weights(model_weight_file)\n",
    "    with open(model_result_file, 'rb') as input_file:\n",
    "        resDict_5kW_trans_RR = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_5kW_trans_RR'\n",
    "model_weight_file = model_name + '_weights'\n",
    "model_result_file = model_name + '_Results.pkl'\n",
    "\n",
    "with open(model_result_file, 'rb') as input_file:\n",
    "    resDict_5kW_trans_RR = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvRoy-H_hCK5"
   },
   "source": [
    "Now we visualize the development of the accuracy and the loss over the training epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1702292066128,
     "user": {
      "displayName": "Andreas Streich",
      "userId": "16674127324132361302"
     },
     "user_tz": -60
    },
    "id": "9pHXivB2guFb",
    "outputId": "dd7937db-0288-4df8-ec32-c3da4833f724"
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(resDict_5kW_trans_RR['epochs'], resDict_5kW_trans_RR['train_loss'],\n",
    "         'g:', label = resDict_5kW_trans_RR['model_name'] +', Training loss')\n",
    "plt.plot(resDict_5kW_trans_RR['epochs'], resDict_5kW_trans_RR['val_loss'],\n",
    "         'g',  label = resDict_5kW_trans_RR['model_name'] +', Validation loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(resDict_5kW_trans_RR['epochs'], resDict_5kW_trans_RR['train_accuracy'],\n",
    "         'k:', label = resDict_5kW_trans_RR['model_name'] +', Training acc')\n",
    "plt.plot(resDict_5kW_trans_RR['epochs'], resDict_5kW_trans_RR['val_accuracy'],\n",
    "         'k',  label = resDict_5kW_trans_RR['model_name'] +', Validation acc')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the results from the previous notebook on LSTM, we can compare the accuracy of e.g. the 1-layer LSTM network with our transformer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_5kW_ae100_1LSTM_ADAM'\n",
    "model_weight_file = model_name + '_weights'\n",
    "model_result_file = model_name + '_Results.pkl'\n",
    "\n",
    "with open(model_result_file, 'rb') as input_file:\n",
    "    resDict_ae100_1LSTM = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(resDict_5kW_trans_RR['epochs'], resDict_5kW_trans_RR['train_accuracy'],\n",
    "         'k:', label = resDict_5kW_trans_RR['model_name'] +', Training acc')\n",
    "plt.plot(resDict_5kW_trans_RR['epochs'], resDict_5kW_trans_RR['val_accuracy'],\n",
    "         'k',  label = resDict_5kW_trans_RR['model_name'] +', Validation acc')\n",
    "\n",
    "plt.plot(resDict_ae100_1LSTM['epochs'], resDict_ae100_1LSTM['train_accuracy'],\n",
    "         'g:', label = resDict_ae100_1LSTM['model_name'] +', Training acc')\n",
    "plt.plot(resDict_ae100_1LSTM['epochs'], resDict_ae100_1LSTM['val_accuracy'],\n",
    "         'g',  label = resDict_ae100_1LSTM['model_name'] +', Validation acc')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the transformer-based solution achieves a good accuracy already after a first training epoch, but then suffers from overfitting. The LSTM-based classifier with adapted, pretrained embeddings takes longer to achieve good performance, but then actually outperforms the transformer-based solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1G0Lblz8wXZpJ2WXHidmwUnvkuFut0tLE",
     "timestamp": 1702284761059
    },
    {
     "file_id": "1cNt2GRcxbNVm-Opgl4eoSRvA2IP2wEMW",
     "timestamp": 1702055474440
    },
    {
     "file_id": "1-tp-joPBYKZFV0j7J81wEo0qkRWSwVY_",
     "timestamp": 1702055033669
    },
    {
     "file_id": "1k0tpSBAFVwL5nlgMq4LOrPm19kuhTqvA",
     "timestamp": 1701875962598
    },
    {
     "file_id": "1Rg2GwZbYjXXF0CXOIelfRVunuS5mVWu6",
     "timestamp": 1700902997204
    },
    {
     "file_id": "17vUvk7t_5jiqHDi0xRcPG4Vu8WA0zlEO",
     "timestamp": 1699259363257
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
