{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosittig/CASAIS/blob/main/CAS_project3_tsittig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyvzr0-vMgdU"
      },
      "source": [
        "# Legal judgement prediction\n",
        "\n",
        "For this project, we will use the **ECHR dataset**, a collection of 11.5K court cases extracted from the public database of the European Court of Human Rights and further annotated by human experts (more info [here](https://www.aclweb.org/anthology/P19-1424/)). You will develop NLP models that, given the facts of a case, predict whether a human rights article or protocol has been violated. We call such problems *binary classification*.\n",
        "\n",
        "We will start from simple logistic regression classifiers that use bag-of-words representations of a court case as features, then move to bidirectional LSTM classifiers with frozen and adaptive embeddings, and conclude with pre-trained and fine-tuned Transformer language models.\n",
        "\n",
        "For those who want to go above and beyond, or simply exercise their NLP classification skills further, it is possible to work on a non-mandatory project extension. Here, you will build models that predict a court case's \"importance score\". This is a value from 1 to 4 that allows legal practitioners to identify pivotal cases. You will address this as a *multi-class classification* problem. But more on this later!\n",
        "\n",
        "All of the binary classification tasks, which are mandatory, are based on notions and code that you have been exposed to through lectures and/or tutorials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiqpAWNoM-Yu"
      },
      "source": [
        "## Preliminary data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OLV5DnUOCtC"
      },
      "source": [
        "Let's begin by loading the dataset. The ECHR dataset is open-source and can be downloaded from [this web page](https://archive.org/details/ECHR-ACL2019), but we are going to load a cleaner version of it, which has been pre-processed for this course.\n",
        "\n",
        "For that, we will need the `datasets` library installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8djOck8sAtoL"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6C5nWs3AuGY"
      },
      "source": [
        "Now we can import the `load_dataset` from `datasets`, as well as the `pandas` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaQR7UOYd1uK"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFZdle6dAyTu"
      },
      "source": [
        "We load the data from the Hugging Face dataset hub and we store it in a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljwlumr3A1mk"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"glnmario/ECHR\")\n",
        "full_data = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Here, 'train' is just the default name for single-partition datasets.\n",
        "# The actual training, development, and test set are defined in the\n",
        "# first column of the dataframe ('partition').\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O-wLx6dA4OG"
      },
      "source": [
        "***Display and inspect the first 5 rows of the dataset.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB6P3-S3A3xd"
      },
      "outputs": [],
      "source": [
        "... # fill in this line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRnoQFWNa6HS"
      },
      "source": [
        "As it is common for datasets used in Machine Learning projects, the dataset is split into 3 partitions: training, development, and test set. The training and development sets contain cases from 1959 through 2013, and the test set from 2014 through 2018.\n",
        "> Note: *It's good practice to never look at the test set during development, as the test set represents the data your Machine Learning system will have to deal with once deployed, which you can't observe at development time. Here, we will keep the test set at hand but you should avoid making any modelling decision based on its content or features. Furthermore, for data which covers a significant period of time (as we have it here), it's best to use the most recent portion of the data as test data, as this will be most similar to the real-world data for which we will use the system.*\n",
        "\n",
        "The sizes of the partitions, in terms of number of court cases, are the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbWjFrWY5AJA"
      },
      "outputs": [],
      "source": [
        "print(\"Training set     \", len(full_data[full_data.partition == \"train\"]))\n",
        "print(\"Development set  \", len(full_data[full_data.partition == \"dev\"]))\n",
        "print(\"Test set         \", len(full_data[full_data.partition == \"test\"]))\n",
        "print(\"Total           \", len(full_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htI2gzVc7dot"
      },
      "source": [
        "Each instance in this dataset is a court case. Each court case is annotated with the following properties (the columns of the dataframe):\n",
        "\n",
        "*   `partition`: a label indicating dataset partition this court case belongs to (\"train\", \"dev\", or \"test\")\n",
        "*   `itemid`: a code which uniquely identifies this court case\n",
        "*   `languageisocode`: an [ISO code](https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes) describing the language in which the case is reported\n",
        "*   `respondent`: the ISO code of the party being sued or tried (respondents are nation states)\n",
        "*   `branch`: the branch of the Court dealing with the case, indicating at which stage of the trial a judgement was made (it can be one out of \"ADMISSIBILITY\", \"CHAMBER\", \"GRANDCHAMBER\", \"COMMITTEE\")\n",
        "*   `date`: the date of the judgement\n",
        "*   `docname`: the title of the court case (for example, \"ERIKSON v. ITALY\")\n",
        "*   `importance`: an \"importance score\" from 1 (key case) to 4 (unimportant), denoting a case's contribution in the development of case-law\n",
        "*   `conclusion`: a short summary of the case conclusion (for example, \"Inadmissible\" or \"Violation of Art. 6-1; No violation of Art. 10\"\n",
        "*   `judges`: the name of the judges\n",
        "*   `text`: the facts brought to the attention of the Court\n",
        "*   `binary_judgement`: a binary label indicating whether an article or protocol was (1) or wasn't (0) violated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VxT6t8l7_aR"
      },
      "outputs": [],
      "source": [
        "full_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m5ncVrtr84R"
      },
      "source": [
        " ### Filter court cases based on length\n",
        " We are now going to filter out from the dataset the court cases with the longest texts. We will do this for two reasons. First, this will speed up the experiments. Second, the Transformer model that we will use at the end of the project has, like most Transormers, a limited *window size*, which cannot fit more than 2048 tokens. This is the maximum sequence length that a Transformer can process at a time.\n",
        "\n",
        "***Set a threshold by inspecting how many data points it tosses out and how balanced the sizes of the different partitions are (see the next four code cells). The threshold should be smaller than 2048, but greater than or equal to 300.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhX193D6tvqX"
      },
      "outputs": [],
      "source": [
        "THRESHOLD = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU-IL-x7typO"
      },
      "source": [
        "Let's look at basic text length statistics and how many court cases are left out when using a certain threshold.\n",
        "\n",
        "First, we measure the length of every text in the dataset. We do this by splitting each text into words as indicated by whitespace characters, and then counting the number of resulting words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcpJD7o1BvZe"
      },
      "outputs": [],
      "source": [
        "# Extract text lengths using whitespaces as a simple criterion to separate words\n",
        "text_lengths = []\n",
        "for text in full_data.text:\n",
        "  word_list = text.split()\n",
        "  num_words = len(word_list)\n",
        "  text_lengths.append(num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf710CrACOUx"
      },
      "source": [
        "Now can plot the distribution of text lengths, marking the threshold with a vertical line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0R1BMMOt9oz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot text lengths\n",
        "plt.hist(text_lengths, bins=100, alpha=0.5)\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Text length (number of words)')\n",
        "\n",
        "# Add a vertical bar corresponding to the threshold\n",
        "plt.axvline(THRESHOLD, color='k', linestyle='dashed', linewidth=1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d263w52ruxzV"
      },
      "source": [
        "As you can see this leaves out quite a few court cases, but it is okay for the purposes of this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD5wMLQ9vBLO"
      },
      "outputs": [],
      "source": [
        "# Add text length as an extra column to the dataset\n",
        "full_data['text_length'] = text_lengths\n",
        "\n",
        "# Calculate how many cases are discarded\n",
        "n_left_out = sum(full_data.text_length > THRESHOLD)\n",
        "print(f\"Omitting {n_left_out} long cases.\")\n",
        "\n",
        "# Filter out court cases with a text length larger than the threshold\n",
        "data = full_data[full_data.text_length <= THRESHOLD]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdMlnqvGwWNC"
      },
      "source": [
        "Let's also make sure the dataset is still reasonably balanced with respect to the training, validation, and test partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su1P_jYRv6yc"
      },
      "outputs": [],
      "source": [
        "print(\"Training set     \", len(data[data.partition == \"train\"]))\n",
        "print(\"Development set  \", len(data[data.partition == \"dev\"]))\n",
        "print(\"Test set         \", len(data[data.partition == \"test\"]))\n",
        "print(\"Total           \", len(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgHqoZHSwlyr"
      },
      "source": [
        "### Data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hEKOBBrBAM1"
      },
      "source": [
        "Now that we have our final version of the dataset, let's visualise the distribution of some of the dataset properties (date, branch, respondent, etc.) to get a sense of the data. What time span does the dataset cover? How many cases make it to the Grand Chamber? Which countries have been sued most often?\n",
        "\n",
        "***Fill in the code for the second plot.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8EhDsw2mLrS"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot number of instances per date\n",
        "plt.subplot(3, 1, 1)\n",
        "sns.countplot(x='date', data=data, palette='viridis')\n",
        "plt.xticks(rotation=90)  # Rotate x-axis labels\n",
        "plt.title('Number of Instances by Date')\n",
        "\n",
        "# Plot number of instances per branch\n",
        "plt.subplot(3, 1, 2)\n",
        "... # fill in this line\n",
        "plt.title('Number of Instances by Branch')\n",
        "\n",
        "# Plot number of instances per top 10 respondents\n",
        "plt.subplot(3, 1, 3)\n",
        "top_respondents = data['respondent'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_respondents.index, y=top_respondents.values, palette='colorblind')\n",
        "plt.title('Number of Instances by Top 10 Respondents')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zuMXXbqFys4"
      },
      "source": [
        "Let's now look at how many cases in this dataset actually resulted in violations of human rights articles or protocols. This is typically called the *class label distribution*. It will give us an idea of the dataset *class balance* (or *class imbalance*), an important property to look out for when making modelling and evaluation decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9Wbw_a8DQmu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "# Plot binary class label distribution per partition\n",
        "sns.countplot(\n",
        "    x='partition',\n",
        "    hue='binary_judgement',\n",
        "    data=data,\n",
        "    palette='colorblind',\n",
        "    order=['train', 'dev', 'test']\n",
        ")\n",
        "\n",
        "# Annotate plot\n",
        "plt.legend(title='Judgement', labels=['0: no violation', '1: violation'])\n",
        "plt.title('Distribution of Binary Judgement Labels for Each Dataset Partition')\n",
        "plt.xlabel('Partition')\n",
        "plt.ylabel('Number of Cases')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ps8GzcG2K2"
      },
      "source": [
        "Finally, let's look at the class distribution of importance scores. Remember: importance scores range from 1 (key case) to 4 (unimportant).\n",
        "\n",
        "***Write code that plots the class distribution per data partition.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u58tf6MNHA09"
      },
      "outputs": [],
      "source": [
        "# Plot importance score distribution per partition\n",
        "\n",
        "# ... # write the code snippet that plots class distribution by partition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPLkuqCPN0Zq"
      },
      "source": [
        "## Binary Judgement Prediction with Bag of Words\n",
        "\n",
        "Let's finally start with the task of predicting the outcome of a case given the text describing the main facts brought to the attention of the court. As we have just seen, each court case is annotated with a binary judgement label: whether the offendant has (label 1) or has not (label 0) violated any human rights article or protocol. This is a similar scenario to the sentiment classification task you have worked on previously in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8zEO4zQzLfa"
      },
      "source": [
        "### Set-up\n",
        "First, we load the necessary python libraries. Similarly to the sentiment classification example, we will use `keras` and `tensorflow`.\n",
        "\n",
        "***Fix the random seed of `tensorflow` and `numpy` to ensure reproducibility.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04CjFB0LN8_H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.layers import Input, TextVectorization, Embedding, Conv1D, MaxPooling1D, Flatten, LSTM, Bidirectional\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "# Initialize random number generators to ensure reproducibility\n",
        "... # fill in this line\n",
        "... # fill in this line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "akmttMJ-LKM2"
      },
      "outputs": [],
      "source": [
        "# @title Convenience functions: prepare data splits in scikit-friendly format\n",
        "# @markdown You don't need to read the code in this cell, but please make sure you execute it.\n",
        "\n",
        "def load_input_from_ECHR_dataset(dataframe):\n",
        "    # Input: text\n",
        "    X_train = data[data.partition == 'train'].text.to_list()\n",
        "    X_val = data[data.partition == 'dev'].text.to_list()\n",
        "    X_test = data[data.partition == 'test'].text.to_list()\n",
        "    return X_train, X_val, X_test\n",
        "\n",
        "def load_binary_output_from_ECHR_dataset(dataframe):\n",
        "    # Binary output: violation judgement\n",
        "    y_train_binary = data[data.partition == 'train'].binary_judgement.to_numpy()\n",
        "    y_val_binary = data[data.partition == 'dev'].binary_judgement.to_numpy()\n",
        "    y_test_binary = data[data.partition == 'test'].binary_judgement.to_numpy()\n",
        "    return y_train_binary, y_val_binary, y_test_binary\n",
        "\n",
        "def load_regression_output_from_ECHR_dataset(dataframe):\n",
        "    # Regression output: case importance score\n",
        "    y_train_regression = data[data.partition == 'train'].importance.astype(float).to_numpy()\n",
        "    y_val_regression = data[data.partition == 'dev'].importance.astype(float).to_numpy()\n",
        "    y_test_regression = data[data.partition == 'test'].importance.astype(float).to_numpy()\n",
        "    return y_train_regression, y_val_regression, y_test_regression\n",
        "\n",
        "def load_multiclass_output_from_ECHR_dataset(dataframe):\n",
        "    # Multiclass output: case importance label\n",
        "    y_train_multiclass = data[data.partition == 'train'].importance.to_numpy()\n",
        "    y_val_multiclass = data[data.partition == 'dev'].importance.to_numpy()\n",
        "    y_test_multiclass = data[data.partition == 'test'].importance.to_numpy()\n",
        "    return y_train_multiclass, y_val_multiclass, y_test_multiclass\n",
        "\n",
        "def load_ECHR_dataset_for_binary_judgement_classification(dataframe, for_tensorflow=False):\n",
        "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
        "    y_train, y_val, y_test = load_binary_output_from_ECHR_dataset(dataframe)\n",
        "    if for_tensorflow:\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "    else:\n",
        "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
        "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
        "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "def load_ECHR_dataset_for_case_importance_regression(dataframe, for_tensorflow=False):\n",
        "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
        "    y_train, y_val, y_test = load_regression_output_from_ECHR_dataset(dataframe)\n",
        "    if for_tensorflow:\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "    else:\n",
        "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
        "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
        "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
        "        return train_ds, val_ds, test_ds\n",
        "\n",
        "def load_ECHR_dataset_for_case_importance_classification(dataframe, for_tensorflow=False):\n",
        "    X_train, X_val, X_test = load_input_from_ECHR_dataset(dataframe)\n",
        "    y_train, y_val, y_test = load_multiclass_output_from_ECHR_dataset(dataframe)\n",
        "    if for_tensorflow:\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "    else:\n",
        "        train_ds = {\"texts\": X_train, \"labels\": y_train}\n",
        "        val_ds = {\"texts\": X_val, \"labels\": y_val}\n",
        "        test_ds = {\"texts\": X_test, \"labels\": y_test}\n",
        "    return train_ds, val_ds, test_ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWPBm8cx3_6n"
      },
      "source": [
        "### Loading the data\n",
        "\n",
        "We now load the data in needed for the binary classification task in a model-friendly format, using some convenience functions defined in the cell above. As we have seen, the ECHR dataset comes with a predefined train-validation-test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmF_K3394kl6"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds, test_ds = load_ECHR_dataset_for_binary_judgement_classification(data, for_tensorflow=True)\n",
        "\n",
        "# Print 3 examples from the dataset\n",
        "for example, label in train_ds.take(3):\n",
        "  print(\"Input: \", example)\n",
        "  print(10*\".\")\n",
        "  print('Target labels: ', label)\n",
        "  print(50*\"-\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh-tmuIii6YT"
      },
      "source": [
        "### Fit and evaluate\n",
        "\n",
        "The following piece of code defines a function that trains (fits) a model on the training data and evaluates it on the development set. It then returns the training and validation accuracy obtained by the model at training epoch.\n",
        "\n",
        "Please take some time to read this code and to understand all of its components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ienyT4zmi4qE"
      },
      "outputs": [],
      "source": [
        "def fit_and_eval_binary_classifier(\n",
        "    train_ds,\n",
        "    val_ds,\n",
        "    model,\n",
        "    learning_rate,\n",
        "    buffer_size,\n",
        "    batch_size,\n",
        "    n_epochs,\n",
        "    patience_n_epochs=5\n",
        "    ):\n",
        "\n",
        "    # compile\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        optimizer=Adam(learning_rate=learning_rate)\n",
        "    )\n",
        "\n",
        "    # preliminaries\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "    tf.config.run_functions_eagerly(True)\n",
        "\n",
        "    # train\n",
        "    history = model.fit(\n",
        "        train_ds.shuffle(buffer_size=buffer_size).batch(batch_size),\n",
        "        validation_data=val_ds.batch(batch_size),\n",
        "        epochs=n_epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[EarlyStopping(\n",
        "            monitor='val_accuracy', patience=patience_n_epochs, verbose=False, restore_best_weights=True\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    return history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqVzBB3NM0Y-"
      },
      "source": [
        "Now that the data is loaded and the training and evaluation procedure is in place, we can move to modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvVK2m1-UdLr"
      },
      "source": [
        "### Creating Bag-of-Words text representations\n",
        "\n",
        "We will use [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) to obtain bag-of-words representations of texts.\n",
        "\n",
        "These representations will depend on two main parameters:\n",
        "* the vocabulary size `VOCAB_SIZE`, which limits the number of word considered to the `VOCAB_SIZE` most frequent ones\n",
        "* the type of bag-of-words representation: based on raw word counts (`count`) or on word counts weighed by inverse document frequency (`tf-idf`)\n",
        "\n",
        "***Write code to create count-based and tf-idf text representations.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpGaUqveWW5w"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# Create count-based features\n",
        "# ----------------------------\n",
        "encoder_bow_count = ... # fill in this line\n",
        "... # fill in this line\n",
        "\n",
        "# Create tf-idf features\n",
        "# ----------------------------\n",
        "encoder_bow_tfidf = ... # fill in this line\n",
        "... # fill in this line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBqSJCRpYaPa"
      },
      "outputs": [],
      "source": [
        "# Let's take a peak at the vocabulary\n",
        "vocab = np.array(encoder_bow_count.get_vocabulary())\n",
        "vocab[:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MpkSFNOXQpu"
      },
      "source": [
        "### Binary classifier 1\n",
        "As a first model, we will implement a logistic regression classifier with count-based BOW representations.\n",
        "\n",
        "***Write code to define the model architecture, the training obectives, and the evaluation metric.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXHLHD_OZasK"
      },
      "outputs": [],
      "source": [
        "# Define main hyperparameters\n",
        "# --------------------------------------------------------------\n",
        "LEARNING_RATE = 0.005\n",
        "N_EPOCHS = 20\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "# --------------------------------------------------------------\n",
        "binary_classifier_1 = Sequential(\n",
        "    name = f'Logistic regression, count-based BOW, |V| = {VOCAB_SIZE}'\n",
        ")\n",
        "# binary_classifier_1.add(...)  # fill in this line\n",
        "# binary_classifier_1.add(...)  # fill in this line\n",
        "# binary_classifier_1.add(...)  # fill in this line\n",
        "\n",
        "\n",
        "# Define training objective, evaluation metric, and optimizer\n",
        "# --------------------------------------------------------------\n",
        "binary_classifier_1.compile(\n",
        "    loss='...', # fill in this line\n",
        "    metrics=['...'], # fill in this line\n",
        "    optimizer=Adam(learning_rate=LEARNING_RATE)\n",
        ")\n",
        "print(binary_classifier_1.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__5odlYuN4HS"
      },
      "source": [
        "Let's fit and evaluate this first classifier. This runs quickly, but if you want, you can skip the next two cells and directly load a pre-trained model with its training history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf-61emXvPAd"
      },
      "outputs": [],
      "source": [
        "# fit_and_eval_binary_classifier returns the training and validation accuracy scores over epochs\n",
        "train_acc_model_1, val_acc_model_1, =  fit_and_eval_binary_classifier(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    model=binary_classifier_1,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    patience_n_epochs=N_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create an output directory\n",
        "!mkdir models_BoW\n",
        "\n",
        "# save classifier\n",
        "binary_classifier_1.save('models_BoW/logistic_regression_count_based_BOW_V_1000.keras')\n",
        "\n",
        "# save training history\n",
        "np.save('models_BoW/logistic_regression_count_based_BOW_V_1000.history.npy', history_classifier_1)\n"
      ],
      "metadata": {
        "id": "EiPkcZMv_Trw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can load a pre-trained classifier and its training history. The code below assumes you have uploaded `models_BoW.zip` onto the (Colab) file system."
      ],
      "metadata": {
        "id": "sciwswGA_ZGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip models_BoW.zip\n",
        "\n",
        "# binary_classifier_1 = load_model('models_BoW/logistic_regression_count_based_BOW_V_1000.keras')\n",
        "# history_classifier_1 = np.load('models_BoW/logistic_regression_count_based_BOW_V_1000.history.npy', allow_pickle='TRUE').item()"
      ],
      "metadata": {
        "id": "YNsH4NDa_aBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_model_1 = history_classifier_1['accuracy']\n",
        "val_acc_model_1 = history_classifier_1['val_accuracy']"
      ],
      "metadata": {
        "id": "ljcDXM0A_lMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y31x5434Gni7"
      },
      "source": [
        "These are its training and validation accuracy over epochs. (Note that the model stops training after `patience_n_epochs` where its loss doesn't improve. We set this value equal to the number of epochs, so the model completes them all. However, you can set this parameter to a lower value for more efficient training. This is what you'd likely do in practice.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw4-twUOyp1x"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    range(1, len(train_acc_model_1) + 1),  # the epochs for the x-axis\n",
        "    train_acc_model_1,  # the training accuracy\n",
        "    'b:',  # for dotted blue line\n",
        "    label='Logreg count-based BOW, Training acc'\n",
        ")\n",
        "plt.plot(\n",
        "    range(1, len(val_acc_model_1) + 1),  # the epochs for the x-axis\n",
        "    val_acc_model_1,  # the validation accuracy\n",
        "    'b',  # for dense blue line\n",
        "    label='Logreg count-based BOW, Validation acc'\n",
        ")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS53VDY-akWv"
      },
      "source": [
        "### Binary classifier 2\n",
        "Next is a logistic regression classifier with tfidf-based BOW representations.\n",
        "\n",
        "\n",
        "***Write code to define the model architecture, the training obectives, and the evaluation metric.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cpz7G3_apkG"
      },
      "outputs": [],
      "source": [
        "# Define main hyperparameters\n",
        "# --------------------------------------------------------------\n",
        "LEARNING_RATE = 0.005\n",
        "N_EPOCHS = 20\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "# --------------------------------------------------------------\n",
        "binary_classifier_2 = Sequential(\n",
        "    name = f'Logistic regression, tfidf-based BOW, |V| = {VOCAB_SIZE}'\n",
        ")\n",
        "binary_classifier_2.add(...)  # fill in this line\n",
        "binary_classifier_2.add(...)  # fill in this line\n",
        "binary_classifier_2.add(...)  # fill in this line\n",
        "\n",
        "\n",
        "# Define training objective, evaluation metric, and optimizer\n",
        "# --------------------------------------------------------------\n",
        "binary_classifier_2.compile(\n",
        "    loss='...',  # fill in this line\n",
        "    metrics=['...'],  # fill in this line\n",
        "    optimizer=Adam(lr=LEARNING_RATE)\n",
        ")\n",
        "print(binary_classifier_2.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F3prC_OO4x6"
      },
      "source": [
        "Fit and evaluate, then plot accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wSAQFxxyPt3"
      },
      "outputs": [],
      "source": [
        "history_classifier_2 =  fit_and_eval_binary_classifier(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    model=binary_classifier_2,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    patience_n_epochs=N_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save classifier\n",
        "binary_classifier_2.save('models_BoW/logistic_regression_tfidf_based_BOW_V_1000.keras')\n",
        "\n",
        "# save training history\n",
        "np.save('models_BoW/logistic_regression_tfidf_based_BOW_V_1000.history.npy', history_classifier_2)\n"
      ],
      "metadata": {
        "id": "NAFX6Ki5_o9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can load a pre-trained classifier and its training history."
      ],
      "metadata": {
        "id": "8MPYL_82_tIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# binary_classifier_2 = load_model('models_BoW/logistic_regression_tfidf_based_BOW_V_1000.keras')\n",
        "# history_classifier_2 = np.load('models_BoW/logistic_regression_tfidf_based_BOW_V_1000.history.npy', allow_pickle='TRUE').item()\n"
      ],
      "metadata": {
        "id": "wpXQhyZs_zCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_model_2 = history_classifier_2['accuracy']\n",
        "val_acc_model_2 = history_classifier_2['val_accuracy']"
      ],
      "metadata": {
        "id": "Kw6LKo0O_2hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0mHJEvczFD2"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    range(1, len(train_acc_model_2) + 1),\n",
        "    train_acc_model_2,\n",
        "    'g:',\n",
        "    label='Logreg tfidf-based BOW, Training acc'\n",
        ")\n",
        "plt.plot(\n",
        "    range(1, len(val_acc_model_2) + 1),\n",
        "    val_acc_model_2,\n",
        "    'g',\n",
        "    label='Logreg tfidf-based BOW, Validation acc'\n",
        ")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8j7sMbm105C"
      },
      "source": [
        "### Model comparison\n",
        "\n",
        "To compare the two models visually, ***plot the training and validation accuracy of the two bag-of-words models.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7Xa0f_E13Og"
      },
      "outputs": [],
      "source": [
        "...  # fill in this line\n",
        "...  # fill in this line\n",
        "...  # fill in this line\n",
        "...  # fill in this line\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGGyNCL2QFI2"
      },
      "source": [
        "***Briefly describe the results.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuPALLwaQO7p"
      },
      "source": [
        "*Enter your response here (two or three sentences should suffice).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNT8BU07eB9-"
      },
      "source": [
        "### Analysis\n",
        "To understand the models' performance beyond the evaluation scores, it is useful to carry out what could be called an *intepretability analysis*.\n",
        "\n",
        "We interpret what the model has learned by analysing its weights.\n",
        "\n",
        "***Write code to extract the weights from the two classifiers above and to obtain the vocabulary entries with the highest weights.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J_BxWyOdXES"
      },
      "outputs": [],
      "source": [
        "..vocab1 = np.array(encoder_bow_count.get_vocabulary())\n",
        "vocab2 = np.array(encoder_bow_tfidf.get_vocabulary())\n",
        "\n",
        "# Extract the classifier weights\n",
        "classifier_1_vocab_weights = ...  # fill in this line\n",
        "classifier_2_vocab_weights = ...  # fill in this line\n",
        "\n",
        "# Sort the weights and get the correspondingly sorted vocabulary indices\n",
        "classifier_1_vocab_weights_sorted = ...  # fill in this line\n",
        "classifier_2_vocab_weights_sorted = ...  # fill in this line\n",
        "\n",
        "# The indices with the largest values indicate which words are most indicative of violations\n",
        "print(\"Words predictive of violations\")\n",
        "print(\"Model 1:\\n\", vocab1[classifier_1_vocab_weights_sorted[-10:]])\n",
        "print()\n",
        "print(\"Model 2:\\n\", vocab2[classifier_2_vocab_weights_sorted[-10:]])\n",
        "\n",
        "# ... and vice versa\n",
        "print(\"\\n\\nWords predictive of absolution\")\n",
        "print(\"Model 1:\\n\", vocab1[classifier_1_vocab_weights_sorted[:10]])\n",
        "print(\"Model 2:\\n\", vocab2[classifier_2_vocab_weights_sorted[:10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6SA_XKqRTfc"
      },
      "source": [
        "Do the words with the highest weights correspond to sensible violation or absolution cues?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYGcXAtLm6fC"
      },
      "source": [
        "## Binary Judgement Prediction with LSTMs\n",
        "\n",
        "As a next model class, we will consider recurrent neural models — in particular, LSTMs. As you have learned, these models are able to take into account the order of words in sentences, which is in principle a big advantage over bag-of-words models. \"The woman sued Switzerland\" is not the same as \"Switzerland sued the woman\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RekaekG5GCS"
      },
      "source": [
        "### BiLSTM with embeddings trained from scratch\n",
        "\n",
        "First, we'll design a simple one-layer bidirectional LSTM with word embeddings learned from scratch.\n",
        "\n",
        "***Write code to create word embeddings for the vocabulary of this dataset.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RBinILwSIea"
      },
      "source": [
        "First, define the right encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHjOEwxl20jX"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 50\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "encoder_embed = ... # fill in this line\n",
        "... # fill in this line\n",
        "\n",
        "# print the vocabulary id of the word \"human\"\n",
        "encoder_embed(\"human\").numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYocTboOSGqq"
      },
      "source": [
        "Then, create the embedding matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67FukSc54zay"
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(\n",
        "    input_dim=...,  # fill in this line\n",
        "    output_dim=...,  # fill in this line\n",
        "    embeddings_initializer=\"uniform\",\n",
        "    trainable=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtBPI5XoTMwM"
      },
      "source": [
        "***Write code to define the model architecture***. Remember, this should include an input layer, encoder and embedding layers, a bidirectional LSTM layer and an output layer. Keep the dimensionality of the LSTM layer low (for example, 16)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBlIrukAnAMD"
      },
      "outputs": [],
      "source": [
        "binary_classifier_3 = Sequential(\n",
        "    name=f\"1-layer BiLSTM classifier, embeddings from scratch)\"\n",
        ")\n",
        "binary_classifier_3.add(...)  # fill in this line\n",
        "binary_classifier_3.add(...)  # fill in this line\n",
        "binary_classifier_3.add(...)  # fill in this line\n",
        "binary_classifier_3.add(...)  # fill in this line\n",
        "binary_classifier_3.add(...)  # fill in this line\n",
        "\n",
        "binary_classifier_3.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-semvdDTjzi"
      },
      "source": [
        "Fit and evaluate. Note: the LSTM takes longer to train than the logistic regression. We set the patience parameter to 3 to avoid redundant epochs. You can also skip the next two cells and load pre-trained model weights directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Src2AvbOlK"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.005\n",
        "BATCH_SIZE = 50\n",
        "BUFFER_SIZE = 10000\n",
        "N_EPOCHS = 20\n",
        "\n",
        "history_classifier_3 =  fit_and_eval_binary_classifier(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    model=binary_classifier_3,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    patience_n_epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create an output directory\n",
        "!mkdir models_LSTM\n",
        "\n",
        "# save classifier\n",
        "binary_classifier_3.save('models_LSTM/1_layer_BiLSTM_embeds_from_scratch.keras')\n",
        "\n",
        "# save training history\n",
        "np.save('models_LSTM/1_layer_BiLSTM_embeds_from_scratch.history.npy', history_classifier_3)\n"
      ],
      "metadata": {
        "id": "_m9mc8j6AT7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can load the pre-trained model. Please upload `models_LSTM.zip` first."
      ],
      "metadata": {
        "id": "-R_NJ_zYAUnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip models_LSTM.zip\n",
        "\n",
        "# binary_classifier_3 = load_model('models_LSTM/1_layer_BiLSTM_embeds_from_scratch.keras')\n",
        "# history_classifier_3 = np.load('models_LSTM/1_layer_BiLSTM_embeds_from_scratch.history.npy', allow_pickle='TRUE').item()"
      ],
      "metadata": {
        "id": "SLwqkMRGAWHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_model_3 = history_classifier_3['accuracy']\n",
        "val_acc_model_3 = history_classifier_3['val_accuracy']"
      ],
      "metadata": {
        "id": "ZryQI_6yAdz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6XXsi7nUsOp"
      },
      "source": [
        "### Deeper network\n",
        "Next, let's try with a deeper two-layer LSTM network. Word embeddings will be still learned from scratch.\n",
        "\n",
        "\n",
        "***Define the full two-layer Bidirectional LSTM in the cell below.***  This is identical to the one-layer model, but with an extra Bidirectional LSTM layer. Again, keep the dimensionality of the LSTM layers low."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Nq7pA9lTX6F"
      },
      "outputs": [],
      "source": [
        "encoder_embed = ... # fill in this line\n",
        "... # fill in this line\n",
        "\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=...,  # fill in this line\n",
        "    output_dim=...,  # fill in this line\n",
        "    embeddings_initializer=\"uniform\",\n",
        "    trainable=True,\n",
        ")\n",
        "\n",
        "binary_classifier_4 = Sequential(\n",
        "    name=f\"2-layer BiLSTM classifier (embeddings from scratch)\"\n",
        ")\n",
        "binary_classifier_4.add(...)  # fill in this line\n",
        "binary_classifier_4.add(...)  # fill in this line\n",
        "binary_classifier_4.add(...)  # fill in this line\n",
        "binary_classifier_4.add(...)  # fill in this line\n",
        "binary_classifier_4.add(...)  # fill in this line\n",
        "binary_classifier_4.add(...)  # fill in this line\n",
        "\n",
        "binary_classifier_4.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWWYavsdUis-"
      },
      "source": [
        "Fit and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o0NIOq-_vtp"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.005\n",
        "BATCH_SIZE = 50\n",
        "BUFFER_SIZE = 10000\n",
        "N_EPOCHS = 20\n",
        "\n",
        "history_classifier_4 =  fit_and_eval_binary_classifier(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    model=binary_classifier_4,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    patience_n_epochs=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save classifier\n",
        "binary_classifier_4.save('models_LSTM/2_layer_BiLSTM_embeds_from_scratch.keras')\n",
        "\n",
        "# save training history\n",
        "np.save('models_LSTM/2_layer_BiLSTM_embeds_from_scratch.history.npy', history_classifier_4)\n"
      ],
      "metadata": {
        "id": "iFAetWGeBFfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can load the pre-trained model."
      ],
      "metadata": {
        "id": "6L3mLROpBXJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# binary_classifier_4 = load_model('models_LSTM/2_layer_BiLSTM_embeds_from_scratch.keras')\n",
        "# history_classifier_4 = np.load('models_LSTM/2_layer_BiLSTM_embeds_from_scratch.history.npy', allow_pickle='TRUE').item()\n"
      ],
      "metadata": {
        "id": "oq5YkVNvBXOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_model_4 = history_classifier_4['accuracy']\n",
        "val_acc_model_4 = history_classifier_4['val_accuracy']"
      ],
      "metadata": {
        "id": "SBWUj9IbBFiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfPZi3szot1"
      },
      "source": [
        "To compare the two bidirectional LSTMs, ***plot the training and validation accuracy of the two models.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6ZUY3a8_166"
      },
      "outputs": [],
      "source": [
        "...  # fill in this line\n",
        "...  # fill in this line\n",
        "...  # fill in this line\n",
        "...  # fill in this line\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMfkPeYfUuYw"
      },
      "source": [
        "### Pre-trained word embeddings\n",
        "\n",
        "The dataset at hand is very domain-specific and not particularly large so it is unlikely that the model will be able learn the general meaning of words. Luckily the network can be initialised with pre-trained word embeddings, which were trained on generalist corpora to capture the meaning of all words in the vocabulary. We will download pre-trained GloVe embeddings of dimensionality 50, trained on a corpus of 6 billion tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0IHqAcxc8As"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBpJlFUPTaWQ"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained GloVe embeddings\n",
        "# ----------------------------------\n",
        "glove_file_path = '/content/glove.6B.50d.txt'\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(glove_file_path) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "for i, word in enumerate(encoder_embed.get_vocabulary()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "        print(word)\n",
        "\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1U2u1zPYieC"
      },
      "source": [
        "#### Frozen embeddings\n",
        "\n",
        "Here, we are going to leave the word embeddings \"frozen\". That is, they will not be updated throughout the training of the LSTM. In this way, the embeddings will remain general representations of word meaning while the rest of the network will specialise for the legal judgement prediction task.\n",
        "\n",
        "***Define a Bidirectional LSTM with frozen, pre-trained word embeddings.*** You can make the LSTM one-layer for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX7nQjOYVzaQ"
      },
      "outputs": [],
      "source": [
        "pretrained_embedding_layer_frozen = Embedding(\n",
        "    input_dim=...,  # fill in this line\n",
        "    output_dim=...,  # fill in this line\n",
        "    embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=...,  # fill in this line\n",
        ")\n",
        "\n",
        "binary_classifier_5 = Sequential(\n",
        "    name=f\"1-layer BiLSTM classifier (frozen pre-trained embeddings)\"\n",
        ")\n",
        "# Fill in the following lines to build the LSTM\n",
        "binary_classifier_5.add(...)\n",
        "binary_classifier_5.add(...)\n",
        "binary_classifier_5.add(...)\n",
        "binary_classifier_5.add(...)\n",
        "binary_classifier_5.add(...)\n",
        "\n",
        "binary_classifier_5.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fba4BGTcWsOI"
      },
      "source": [
        "Fit and evaluate. Alternatively, skip the next two cells and load the pre-trained model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_c6rd_qWVnO"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.005\n",
        "BATCH_SIZE = 50\n",
        "BUFFER_SIZE = 10000\n",
        "N_EPOCHS = 20\n",
        "\n",
        "history_classifier_5 =  fit_and_eval_binary_classifier(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    model=binary_classifier_5,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    n_epochs=N_EPOCHS,\n",
        "    patience_n_epochs=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save classifier\n",
        "binary_classifier_5.save('models_LSTM/1_layer_BiLSTM_embeds_pretrained_frozen.keras')\n",
        "\n",
        "# save training history\n",
        "np.save('models_LSTM/1_layer_BiLSTM_embeds_pretrained_frozen.history.npy', history_classifier_5)"
      ],
      "metadata": {
        "id": "mReP_g97BhnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary_classifier_5 = load_model('models_LSTM/1_layer_BiLSTM_embeds_pretrained_frozen.keras')\n",
        "# history_classifier_5 = np.load('models_LSTM/1_layer_BiLSTM_embeds_pretrained_frozen.history.npy', allow_pickle='TRUE').item()\n"
      ],
      "metadata": {
        "id": "ej6pRwpUBhqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_model_5 = history_classifier_5['accuracy']\n",
        "val_acc_model_5 = history_classifier_5['val_accuracy']"
      ],
      "metadata": {
        "id": "XRf4AmLWBjV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyRoysyvWdmc"
      },
      "source": [
        "#### Adaptive embeddings\n",
        "\n",
        "Now let's unfreeze the embeddings and allow them to be updated throughout training.\n",
        "\n",
        "***Define a Bidirectional LSTM with adaptive embeddings.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wkloKEFWbSq"
      },
      "outputs": [],
      "source": [
        "pretrained_embedding_layer_adaptive = Embedding(\n",
        "    input_dim=...,  # fill in this line\n",
        "    output_dim=...,  # fill in this line\n",
        "    embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=...,  # fill in this line\n",
        ")\n",
        "\n",
        "binary_classifier_6 = Sequential(\n",
        "    name=f\"1-layer BiLSTM classifier (frozen pre-trained embeddings)\"\n",
        ")\n",
        "# Fill in the following lines to build the LSTM\n",
        "binary_classifier_6.add(...)\n",
        "binary_classifier_6.add(...)\n",
        "binary_classifier_6.add(...)\n",
        "binary_classifier_6.add(...)\n",
        "binary_classifier_6.add(...)\n",
        "\n",
        "binary_classifier_6.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M6MjeMDfrwW"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.005\n",
        "BATCH_SIZE = 50\n",
        "BUFFER_SIZE = 10000\n",
        "N_EPOCHS = 20\n",
        "\n",
        "history_classifier_6 =  fit_and_eval_binary_classifier(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    model=binary_classifier_6,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    buffer_size=BUFFER_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    n_epochs=N_EPOCHS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save classifier\n",
        "binary_classifier_6.save('models_LSTM/1_layer_BiLSTM_embeds_pretrained_adaptive.keras')\n",
        "\n",
        "# save training history\n",
        "np.save('models_LSTM/1_layer_BiLSTM_embeds_pretrained_adaptive.history.npy', history_classifier_6)\n"
      ],
      "metadata": {
        "id": "xcgEX-xqBpGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary_classifier_6 = load_model('models_LSTM/1_layer_BiLSTM_embeds_pretrained_adaptive.keras')\n",
        "# history_classifier_6 = np.load('models_LSTM/1_layer_BiLSTM_embeds_pretrained_adaptive.history.npy', allow_pickle='TRUE').item()\n"
      ],
      "metadata": {
        "id": "NTjtRliDBpYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_model_6 = history_classifier_6['accuracy']\n",
        "val_acc_model_6 = history_classifier_6['val_accuracy']"
      ],
      "metadata": {
        "id": "Uox5N721Br5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_abgEOf1yOv"
      },
      "source": [
        "***Plot the training and validation accuracy of the four LSTM models.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNTbi9kLl6EZ"
      },
      "outputs": [],
      "source": [
        "... # fill in this code block\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxrDKW1rYrjD"
      },
      "source": [
        "## Binary Judgement Prediction with Transformer language models\n",
        "\n",
        "The last model class we'll experiment with are Transformer language models. We will *not* train a model from scratch on this dataset because Transformer language models are typically very large networks, with million of parameters, which would likely overfit to the dataset at hand. Instead, we will use a pre-trained language model, an autoregressive Transformer optimised to predict the next word in texts of many different domains.\n",
        "\n",
        "We suggest you use [GPT-neo-125m](https://huggingface.co/EleutherAI/gpt-neo-125m), a model designed to replicate the architecture of OpenAI's GPT-3 in its smallest version (125 million parameters). Feel free to substitute this with another pretrained autoregressive language model from the Hugging Face [model hub](https://huggingface.co/models?sort=trending) but beware of model size.\n",
        "\n",
        "If you are running this notebook on Google colab, *change the runtime type to `T4-GPU` using the dropdown menu on the top right.* After that, you might need to reload the data and the convenience functions defined above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-O41arbH8t"
      },
      "source": [
        "First, let's install and load the necessary python libraries. If you run this notebook in the ETH Jupyter hub, you can directly load the libraries. If not, please install using the cell below and restart the Runtime session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HxMKRWJ6MVh"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sacremoses accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NfLarZhdD7y"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm_notebook as tqdm\n",
        "from sklearn.metrics import accuracy_score, PrecisionRecallDisplay\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, pipeline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Wu71suXkumPc"
      },
      "outputs": [],
      "source": [
        "# @title Convenience functions for Huggingface transformers\n",
        "# @markdown You don't need to read the code in this cell, but please make sure you execute it.\n",
        "\n",
        "def load_classification_model_and_tokenizer(model_name_or_path):\n",
        "    lm = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
        "\n",
        "    # Load the tokenizer suitable for this model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "    if not lm.config.pad_token_id:\n",
        "        lm.config.pad_token_id = lm.config.eos_token_id\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return lm, tokenizer\n",
        "\n",
        "class EHRCDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(self.texts[idx],\n",
        "                                  truncation=True,\n",
        "                                  padding='max_length',\n",
        "                                  max_length=self.max_length,\n",
        "                                  return_attention_mask=True,\n",
        "                                  return_tensors='pt')\n",
        "\n",
        "        item = {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        return item\n",
        "\n",
        "\n",
        "def finetune_lm(model, train_dataset, val_dataset, n_epochs, batch_size, learning_rate, output_dir):\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=n_epochs,\n",
        "        logging_dir=\"./logs\",\n",
        "        load_best_model_at_end=True,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    # Create Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.predictions.argmax(-1), p.label_ids)},\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    return model, trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRdrFO1rbfAY"
      },
      "source": [
        "Load the data in model-friendly format using the convenience functions above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08fyzoJk-Rb3"
      },
      "outputs": [],
      "source": [
        "# Load the data using our convenience functions\n",
        "train_ds, val_ds, test_ds = load_ECHR_dataset_for_binary_judgement_classification(data)\n",
        "\n",
        "# Load the tokenizer suitable for the model model\n",
        "MODEL_NAME = \"EleutherAI/gpt-neo-125m\"\n",
        "lm, tokenizer = load_classification_model_and_tokenizer(MODEL_NAME)\n",
        "\n",
        "# Create dataset and data loaders for training and validation\n",
        "train_dataset = EHRCDataset(train_ds['texts'], train_ds['labels'], tokenizer, max_length=2048)\n",
        "val_dataset = EHRCDataset(val_ds['texts'], val_ds['labels'], tokenizer, max_length=2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-HGCwbddJ7k"
      },
      "source": [
        "### Zero-shot classification and prompting\n",
        "\n",
        "Note that this model is pre-trained on the general language modelling task (predicting the next word in a text) and not on the legal judgement prediction task. This is different from the setup you have seen in the tutorial on pre-trained Transformers. The type of classification we will perform with this model is typically referred to as *zero-shot classification*, meaning that the model is asked to classify by seeing *no* examples from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x951-hZfvbDG"
      },
      "outputs": [],
      "source": [
        "zero_shot_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=lm_name,\n",
        "    device=\"cuda:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWpgbCRyzCQC"
      },
      "source": [
        "Instead of using a `text-classification` pipeline, we are using a `zero-shot-classification` pipeline. These two are almost equivalent except that `zero-shot-classification` doesn't require a hardcoded number of potential classes. They can be chosen at runtime:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PhetOn9vt43"
      },
      "outputs": [],
      "source": [
        "candidate_labels = [\"innocent\", \"guilty\"]\n",
        "label2id = {label: i for i, label in enumerate(candidate_labels)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS3MBXI30jQL"
      },
      "source": [
        "Why should this work? The language model is essentially asked if \"innocent\" is more or less likely to follow the court case text then \"guilty\".\n",
        "\n",
        "But does it work in practice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U14yk7iQxUTC"
      },
      "outputs": [],
      "source": [
        "predictions_binary_classifier_7 = []\n",
        "\n",
        "for text in tqdm(val_ds[\"texts\"]):\n",
        "\n",
        "    # Forward pass of zero-shot classification\n",
        "    result = zero_shot_classifier(\n",
        "        text,\n",
        "        candidate_labels\n",
        "    )\n",
        "\n",
        "    # Get the model prediction (labels ordered according to their probability)\n",
        "    prediction = label2id[result[\"labels\"][0]]\n",
        "    predictions_binary_classifier_7.append(prediction)\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc_classifier7 = accuracy_score(val_ds[\"labels\"], predictions_binary_classifier_7)\n",
        "print(\"\\nAccuracy:\", acc_classifier7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQXdclqh2SJk"
      },
      "source": [
        "To further steer the model towards giving sensible answers, it is good practice to prepend or append a templated string to the input example. In this case, we could for instance use the template \"The party being sued in this court case is\", which makes the model much less surprised to see \"innocent\" or \"guilty\" as continuations and gives the model a context to interpret those continuations as we would like it to. This technique is referred to as *prompting*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB7jqZ7f0gTX"
      },
      "outputs": [],
      "source": [
        "prompt = \"The party being sued in this court case is {}\"\n",
        "candidate_labels = [\"innocent\", \"guilty\"]\n",
        "label2id = {label: i for i, label in enumerate(candidate_labels)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgUN8Eiec2ne"
      },
      "source": [
        "Does this work better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55PwOi173wWt"
      },
      "outputs": [],
      "source": [
        "predictions_binary_classifier_8 = []\n",
        "\n",
        "for text in tqdm(val_ds[\"texts\"]):\n",
        "\n",
        "    # Forward pass of zero-shot classification\n",
        "    result = zero_shot_classifier(\n",
        "        text,\n",
        "        candidate_labels,\n",
        "        hypothesis_template=prompt  # here we prompt the model with our template\n",
        "    )\n",
        "\n",
        "    # Get the model prediction (labels ordered according to their probability)\n",
        "    prediction = label2id[result[\"labels\"][0]]\n",
        "    predictions_binary_classifier_8.append(prediction)\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc_classifier8 = accuracy_score(val_ds[\"labels\"], predictions_binary_classifier_8)\n",
        "print(\"\\nAccuracy:\", acc_classifier8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7I7BA2D5G5f"
      },
      "source": [
        "***Try at least one more combination of prompt and labels and test the corresponding zero-shot classifier.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ETq-NE95GTu"
      },
      "outputs": [],
      "source": [
        "# prompt = \"...\"  # fill in a prompt\n",
        "# candidate_labels = [\"...\", \"...\"]  # fill in potential labels\n",
        "prompt = \"Is this a case of 'violation' of human rights or a case of 'absolution'? It is a case of {}\"\n",
        "candidate_labels = [\"violation\", \"absolution\"]  # fill in potential labels\n",
        "label2id = {label: i for i, label in enumerate(candidate_labels)}\n",
        "\n",
        "predictions_binary_classifier_9 = []\n",
        "\n",
        "for text in tqdm(val_ds[\"texts\"]):\n",
        "  ... # forward pass\n",
        "\n",
        "  ... # get the model prediction\n",
        "\n",
        "\n",
        "... # calculate the accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v06zFnqT5z3p"
      },
      "source": [
        "### Fine-tuning\n",
        "\n",
        "Finally, we fine-tune the pre-trained language model on the binary prediction task. By showing it examples of court cases and supervised labels, we obtain a Transformer model specialized for the judgement prediction task. Note that this might result in the model forgetting previous knowledge and becoming less performant in other tasks, including next-word prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqALSmPee_K"
      },
      "source": [
        "Let's launch the fine-tuning and save the fine-tuned model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Qj9FgUur8F"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 5\n",
        "BATCH_SIZE = 3\n",
        "LEARNING_RATE = 1e-5\n",
        "OUTPUT_DIR = \"/content/lm_for_classification_5ep\"\n",
        "\n",
        "lm_finetuned, lm_trainer = finetune_lm(lm, train_dataset, val_dataset, N_EPOCHS, BATCH_SIZE, LEARNING_RATE, OUTPUT_DIR)\n",
        "\n",
        "# Save or use the trained model as needed\n",
        "lm.save_pretrained(OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0woQtYKek2N"
      },
      "source": [
        "Now we obtain predictions from the model and evaluate its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI56nIt8u2Mr"
      },
      "outputs": [],
      "source": [
        "# List to store predicted labels\n",
        "predictions_binary_classifier_10 = []\n",
        "\n",
        "# Tokenize and predict labels for each example in the dataset\n",
        "for text in val_ds['texts']:\n",
        "\n",
        "    # Tokenize input text\n",
        "    tokenized_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "    # Forward pass\n",
        "    output = lm_finetuned(**tokenized_input)\n",
        "\n",
        "    # Get predicted label\n",
        "    predicted_label = torch.argmax(output.logits, dim=1).item()\n",
        "\n",
        "    # Store predicted label in the list\n",
        "    predictions_binary_classifier_10.append(predicted_label)\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc_classifier10 = accuracy_score(val_ds[\"labels\"], predictions_binary_classifier_10)\n",
        "print(acc_classifier10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also load the fine-tuned model weights."
      ],
      "metadata": {
        "id": "6LFMbA9KCITY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip models_Transformer.zip"
      ],
      "metadata": {
        "id": "1kueuoXiCGsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lm_finetuned, tokenizer = load_classification_model_and_tokenizer(\"models_Transformer/checkpoint-2204\")"
      ],
      "metadata": {
        "id": "eWUXylZiCQ4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "# lm_finetuned.to(device)\n",
        "\n",
        "# # List to store predicted labels\n",
        "# predictions_binary_classifier_10 = []\n",
        "\n",
        "# # Tokenize and predict labels for each example in the dataset\n",
        "# for text in val_ds['texts']:\n",
        "\n",
        "#     # Tokenize input text\n",
        "#     tokenized_input = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "#     # Forward pass\n",
        "#     output = lm_finetuned(**tokenized_input)\n",
        "\n",
        "#     # Get predicted label\n",
        "#     predicted_label = torch.argmax(output.logits, dim=1).item()\n",
        "\n",
        "#     # Store predicted label in the list\n",
        "#     predictions_binary_classifier_10.append(predicted_label)\n",
        "\n",
        "# # Calculate the accuracy\n",
        "# acc_classifier10 = accuracy_score(val_ds[\"labels\"], predictions_binary_classifier_10)\n",
        "# print(acc_classifier10)"
      ],
      "metadata": {
        "id": "aJM6rxmZCQ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TswN7-GF6YEQ"
      },
      "source": [
        "## Evaluate on the test set\n",
        "\n",
        "You have compared at least 10 different classifiers so far. ***Now evaluate the best 3 on the test set and report their accuracy.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9g5VFwX6t8q"
      },
      "outputs": [],
      "source": [
        "# Load test set\n",
        "\n",
        "_, _, test_set = load_ECHR_dataset_for_binary_judgement_classification(data)\n",
        "\n",
        "test_documents = test_set['texts']\n",
        "test_labels = test_set['labels']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFAoF7NUjNVc"
      },
      "source": [
        "Example evaluation with logistic regression classifiers and LSTMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrurLEL5jOw5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Make prediction for the test set sentences\n",
        "predictions = binary_classifier_1.predict(\n",
        "    test_documents\n",
        ")\n",
        "\n",
        "# Turn predicted probabilities into binary classification scores\n",
        "binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\n",
        "\n",
        "# Evaluate model by comparing its prediction to the gold labels\n",
        "report = classification_report(\n",
        "    y_true=test_labels,\n",
        "    y_pred=binary_predictions\n",
        ")\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uqrThNmjQ0A"
      },
      "source": [
        "Example evaluation with Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl_ELcdPjSxo"
      },
      "outputs": [],
      "source": [
        "prompt = \"Is this a case of 'violation' of human rights or a case of 'absolution'? It is a case of {}\"\n",
        "candidate_labels = [\"violation\", \"absolution\"]  # fill in potential labels\n",
        "label2id = {label: i for i, label in enumerate(candidate_labels)}\n",
        "\n",
        "zero_shot_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"EleutherAI/gpt-neo-125m\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Make predictions with Transformers\n",
        "binary_predictions = []\n",
        "\n",
        "for text in test_documents:\n",
        "    # Forward pass of zero-shot classification\n",
        "    result = zero_shot_classifier(\n",
        "        text,\n",
        "        candidate_labels,\n",
        "        hypothesis_template=prompt\n",
        "    )\n",
        "\n",
        "    # Get the model prediction (labels ordered according to their probability)\n",
        "    prediction = label2id[result[\"labels\"][0]]\n",
        "    binary_predictions.append(prediction)\n",
        "\n",
        "# Evaluation report\n",
        "report = classification_report(\n",
        "    y_true=test_labels,\n",
        "    y_pred=binary_predictions\n",
        ")\n",
        "\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7XlJi-Byx1p"
      },
      "source": [
        "## [Optional] Case importance prediction\n",
        "\n",
        "The main task of this project was binary legal judgement classification but each court case in the ECHR dataset is also annotated with importance scores, a value from 1 to 4 that allows legal practitioners to identify pivotal cases.\n",
        "\n",
        "> Note: Importance scores can be thought of as values on a continuous scale from 1 to 4, or they can be considered as four separate classes, each with its specific meaning. Depending on which interpretation we decide to go with, predicting importance scores can be cast as a:\n",
        "*   *regression task*: predicting a continous score from 1 to 4\n",
        "*   *multi-class classification*: predicting a categorical label out ot 4 options\n",
        "\n",
        "**Your (optional and open-ended) task is now to train and compare multi-class classifiers that predict the importance score of a court case.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1i-KQzty6Vb"
      },
      "outputs": [],
      "source": [
        "# ..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}