{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosittig/CASAIS/blob/main/W2_4_2_Pretrained_Language_Generation_4colab_OUTPUT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMAjhRtJ_nkL"
      },
      "source": [
        "# Language Generation using Pretrained Models\n",
        "\n",
        "In this notebook, we will finally look at language generation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkRu2VrfbTWP"
      },
      "source": [
        "## Preparations\n",
        "Import packages, and customize settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95xS7aiPbg1k"
      },
      "outputs": [],
      "source": [
        "# Libraries for deep learning. In the background, we will use torch / pytorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Libraries from huggingface to easily interact with pretrained models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC6ziHmqbuwn"
      },
      "outputs": [],
      "source": [
        "# general Python libraries:\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZVLWuPtCwew"
      },
      "outputs": [],
      "source": [
        "# make sure the entire text is output:\n",
        "pd.set_option('display.max_colwidth', 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BSKQ8gcbkff"
      },
      "outputs": [],
      "source": [
        "# what device is this notebook running on?\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBcH7sUcvN_"
      },
      "source": [
        "For this notebook, we will use the `GPT2` model, that has been open-sources by openAI. We use the corresponding tokenizer and causal language model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJaGtOz2_0Yl"
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"\n",
        "# if you have a powerful computer and want to use a larger model:\n",
        "# model_name = \"gpt2-xl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8-35HL__xyd"
      },
      "source": [
        "## Greedy Search Decoding\n",
        "\n",
        "We start with an example for greedy decoding. While there is an easy way of using this decoding strategy via the `generate` function of the `model` with the pretrained parameters, we start with a more granular look at the sampling method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxR_JK2-iMbg"
      },
      "outputs": [],
      "source": [
        "input_txt = \"Transformers are the\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fgSkC9XiO3m"
      },
      "source": [
        "### An in-depth Look at Greedy Search\n",
        "Starting from the input text \"Transformers are the\", we repeatedly call the model. As a result of the model, we get (among other things) the logits, the non-normalized scores by the model for each of the tokens. We then normalize these scores to probabilities, and store the tokens with the highest probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3HbwHhgB_ev"
      },
      "outputs": [],
      "source": [
        "iterations = []\n",
        "n_steps = 8\n",
        "choices_per_step = 5\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(n_steps):\n",
        "        iteration = dict()\n",
        "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
        "        output = model(input_ids=input_ids)\n",
        "        # Select logits of the first batch and the last token and apply softmax\n",
        "        next_token_logits = output.logits[0, -1, :]\n",
        "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
        "        # Store tokens with highest probabilities\n",
        "        for choice_idx in range(choices_per_step):\n",
        "            token_id = sorted_ids[choice_idx]\n",
        "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
        "            token_choice = (\n",
        "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
        "            )\n",
        "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
        "        # Append predicted next token to input\n",
        "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
        "        iterations.append(iteration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBlr1YpRhAjp"
      },
      "source": [
        "Now let us look at the most probable tokens of every iteration step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "f4T6ZUNbdju_",
        "outputId": "92b356bc-e531-4358-91d9-f9ac9003d3f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     Input           Choice 1  \\\n",
              "0                                     Transformers are the       most (9.76%)   \n",
              "1                                Transformers are the most    common (22.90%)   \n",
              "2                         Transformers are the most common      type (15.06%)   \n",
              "3                    Transformers are the most common type        of (83.13%)   \n",
              "4                 Transformers are the most common type of   particle (1.55%)   \n",
              "5        Transformers are the most common type of particle         . (14.26%)   \n",
              "6       Transformers are the most common type of particle.      They (17.48%)   \n",
              "7  Transformers are the most common type of particle. They       are (38.78%)   \n",
              "\n",
              "            Choice 2            Choice 3          Choice 4  \\\n",
              "0       same (2.94%)        only (2.87%)      best (2.38%)   \n",
              "1   powerful (6.88%)   important (6.32%)   popular (3.95%)   \n",
              "2      types (3.31%)        form (1.91%)       way (1.89%)   \n",
              "3         in (3.16%)           . (1.92%)         , (1.63%)   \n",
              "4     object (1.02%)       light (0.71%)    energy (0.67%)   \n",
              "5        in (11.57%)       that (10.19%)         , (9.57%)   \n",
              "6        \\n (15.19%)         The (7.06%)     These (3.09%)   \n",
              "7       have (8.14%)         can (7.98%)       're (5.04%)   \n",
              "\n",
              "               Choice 5  \n",
              "0         first (1.77%)  \n",
              "1      commonly (2.14%)  \n",
              "2           and (1.49%)  \n",
              "3           for (0.88%)  \n",
              "4       objects (0.66%)  \n",
              "5   accelerator (5.81%)  \n",
              "6            In (3.07%)  \n",
              "7       consist (1.57%)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de22121b-5df1-4052-828d-b08a3211afa3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input</th>\n",
              "      <th>Choice 1</th>\n",
              "      <th>Choice 2</th>\n",
              "      <th>Choice 3</th>\n",
              "      <th>Choice 4</th>\n",
              "      <th>Choice 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Transformers are the</td>\n",
              "      <td>most (9.76%)</td>\n",
              "      <td>same (2.94%)</td>\n",
              "      <td>only (2.87%)</td>\n",
              "      <td>best (2.38%)</td>\n",
              "      <td>first (1.77%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Transformers are the most</td>\n",
              "      <td>common (22.90%)</td>\n",
              "      <td>powerful (6.88%)</td>\n",
              "      <td>important (6.32%)</td>\n",
              "      <td>popular (3.95%)</td>\n",
              "      <td>commonly (2.14%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Transformers are the most common</td>\n",
              "      <td>type (15.06%)</td>\n",
              "      <td>types (3.31%)</td>\n",
              "      <td>form (1.91%)</td>\n",
              "      <td>way (1.89%)</td>\n",
              "      <td>and (1.49%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Transformers are the most common type</td>\n",
              "      <td>of (83.13%)</td>\n",
              "      <td>in (3.16%)</td>\n",
              "      <td>. (1.92%)</td>\n",
              "      <td>, (1.63%)</td>\n",
              "      <td>for (0.88%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Transformers are the most common type of</td>\n",
              "      <td>particle (1.55%)</td>\n",
              "      <td>object (1.02%)</td>\n",
              "      <td>light (0.71%)</td>\n",
              "      <td>energy (0.67%)</td>\n",
              "      <td>objects (0.66%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Transformers are the most common type of particle</td>\n",
              "      <td>. (14.26%)</td>\n",
              "      <td>in (11.57%)</td>\n",
              "      <td>that (10.19%)</td>\n",
              "      <td>, (9.57%)</td>\n",
              "      <td>accelerator (5.81%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Transformers are the most common type of particle.</td>\n",
              "      <td>They (17.48%)</td>\n",
              "      <td>\\n (15.19%)</td>\n",
              "      <td>The (7.06%)</td>\n",
              "      <td>These (3.09%)</td>\n",
              "      <td>In (3.07%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Transformers are the most common type of particle. They</td>\n",
              "      <td>are (38.78%)</td>\n",
              "      <td>have (8.14%)</td>\n",
              "      <td>can (7.98%)</td>\n",
              "      <td>'re (5.04%)</td>\n",
              "      <td>consist (1.57%)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de22121b-5df1-4052-828d-b08a3211afa3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de22121b-5df1-4052-828d-b08a3211afa3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de22121b-5df1-4052-828d-b08a3211afa3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-489a6e07-b914-44cc-b6c5-6ee83521a0a2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-489a6e07-b914-44cc-b6c5-6ee83521a0a2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-489a6e07-b914-44cc-b6c5-6ee83521a0a2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Transformers are the most\",\n          \"Transformers are the most common type of particle\",\n          \"Transformers are the\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" common (22.90%)\",\n          \". (14.26%)\",\n          \" most (9.76%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" powerful (6.88%)\",\n          \" in (11.57%)\",\n          \" same (2.94%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" important (6.32%)\",\n          \" that (10.19%)\",\n          \" only (2.87%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" popular (3.95%)\",\n          \", (9.57%)\",\n          \" best (2.38%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Choice 5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \" commonly (2.14%)\",\n          \" accelerator (5.81%)\",\n          \" first (1.77%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "pd.DataFrame(iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpbO0oNhC_yK"
      },
      "source": [
        "### Using the `generate()` Function\n",
        "The Huggingface transformer model has a function `generate()` to generate texts, and allows us to specify the methods to be used for the text generation. Without futher arguments, the greedy search is implemented:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbFBAi67CyiX",
        "outputId": "d2d0f51f-fad1-41e5-bc8e-e5c14261ecaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are the most common type of particle. They are\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False,\n",
        "                        pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vThXwRbSDhRi"
      },
      "source": [
        "We can also try to reproduce the unicorn story presented along with GPT-2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tHI21DEDQcy",
        "outputId": "ea3e713f-ec8b-4285-a516-974b3ff4faca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "\n",
            "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n"
          ]
        }
      ],
      "source": [
        "max_length = 128\n",
        "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
        "a herd of unicorns living in a remote, previously unexplored \\\n",
        "valley, in the Andes Mountains. Even more surprising to the \\\n",
        "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
        "\"\"\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False,\n",
        "                               pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(output_greedy[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBLNIXSjDtcL"
      },
      "source": [
        "The text is different from what openAI has reported when releasing GPT-2, but at first sight, it seems plausible and is grammatically correct. The involved researcher gets a name, and he works at a different university, however also in California. The later part of the generated text are very repretitive - looks like GPT-2 is highly convinced that unicorns are very intelligent :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj5WgmrfimWV"
      },
      "source": [
        "### GPT as Calculator?\n",
        "We can also try to make GPT-2 calculate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgzSj8ncFI4n",
        "outputId": "c5cf1a15-7c57-42c9-da51-d3c06dd4e078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 + 8 = 13\n",
            "2 + 7 = 9\n",
            "13 - 5 = 8\n",
            "2 * 5 = 10\n",
            "5 + 7 =\n",
            "10 - 4 = 11\n",
            "11 - 3 = 12\n",
            "12 - 2 = 13\n",
            "13 - 1 = 14\n",
            "14 - 0 = 15\n",
            "15 - 0 = 16\n",
            "16 - 1 =\n"
          ]
        }
      ],
      "source": [
        "max_length_math = 70\n",
        "input_txt_math1 = \"\"\"\n",
        "5 + 8 = 13\n",
        "2 + 7 = 9\n",
        "13 - 5 = 8\n",
        "2 * 5 = 10\n",
        "5 + 7 =\n",
        "\"\"\"\n",
        "input_ids_math1 = tokenizer(input_txt_math1, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy_math1 = model.generate(input_ids_math1, max_length=max_length_math,\n",
        "                               do_sample=False,\n",
        "                               pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(output_greedy_math1[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sCYwCrKiw-8"
      },
      "source": [
        "Unfortunately, this is completely wrong. Even when we're restricting ourselves to the addition, our model completely fails:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF7_DCcxFaHP",
        "outputId": "d5a74435-f16f-46f4-fa82-7d3b89f6116b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5 + 8 = 13\n",
            "2 + 7 = 9\n",
            "5 + 7 =\n",
            "\n",
            "6 + 7 =\n",
            "\n",
            "7 + 7 =\n",
            "\n",
            "8 + 7 =\n",
            "\n",
            "9 + 7 =\n",
            "\n",
            "10 + 7 =\n",
            "\n",
            "11 + 7 =\n",
            "\n",
            "12 + 7 =\n",
            "\n",
            "13 + 7 =\n",
            "\n",
            "14 + 7\n"
          ]
        }
      ],
      "source": [
        "input_txt_math2 = \"\"\"\n",
        "5 + 8 = 13\n",
        "2 + 7 = 9\n",
        "5 + 7 =\n",
        "\"\"\"\n",
        "input_ids_math2 = tokenizer(input_txt_math2, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy_math2 = model.generate(input_ids_math2, max_length=max_length_math,\n",
        "                               do_sample=False,\n",
        "                               pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(output_greedy_math2[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvafV9OFFtmZ"
      },
      "source": [
        "It's been known for a while that the large language models are bad in doing calcations. We revert to the fiction story about the Andine unicors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSbIYwxVjWCz"
      },
      "source": [
        "## Beam Search\n",
        "Next, we look at the beam search strategy, which keeps a set of most probable partial solutions.\n",
        "\n",
        "First, we define two functions to compute the log-probability of a sequence from the logits we get from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pLbUqv1GRSw"
      },
      "outputs": [],
      "source": [
        "def log_probs_from_logits(logits, labels):\n",
        "    logp = F.log_softmax(logits, dim=-1)\n",
        "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
        "    return logp_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uayQ-vksGR60"
      },
      "outputs": [],
      "source": [
        "def sequence_logprob(model, labels, input_len=0):\n",
        "    with torch.no_grad():\n",
        "        output = model(labels)\n",
        "        log_probs = log_probs_from_logits(\n",
        "            output.logits[:, :-1, :], labels[:, 1:])\n",
        "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
        "    return seq_log_prob.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8UlZaQhjr93"
      },
      "source": [
        "Let us calculate the log-probability of the greedy output we've obtained before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "YYaLksa9GTU2",
        "outputId": "2b7bf6bd-a9dc-43d4-c82f-e005b1fc8a62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\\n\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n",
        "tokenizer.decode(output_greedy[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nlog-prob: {logp:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGmwG13t_TBB",
        "outputId": "6cc7fbac-6884-455f-8b59-0b4c2b71daf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "log-prob: -83.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz3xez8ujwD-"
      },
      "source": [
        "Now, let's generate a text continuation using beam search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "oayVSG_CGU_T",
        "outputId": "def15104-8cc0-4c32-ca85-6f8ba51a4a1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\\nThe researchers, from the University of California, San Diego, and the University of California, Santa Cruz, found that the unicorns were able to communicate with each other in a way that was similar to that of human speech.\\n\\n\\n\"The unicorns were able to communicate with each other in a way that was similar to that of human speech,\" said study co-lead author Dr. David J.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
        "                             do_sample=False,\n",
        "                             pad_token_id=tokenizer.eos_token_id)\n",
        "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
        "tokenizer.decode(output_beam[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nlog-prob: {logp:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lWQZx3o_LWK",
        "outputId": "391f54d4-64c4-4ed6-9437-2e5d5fe83d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "log-prob: -78.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UA8dXLvGq6d"
      },
      "source": [
        "Tracking several potential continuations of the start sentence, we get a sentence that has a higher overall probability, and also sounds much more natural - but the part that the unicorns communicate in a way similar to that of human speech is still there. We can force the `generate` function not to produce repeated `n`-grams (i.e., combinations of `n` words that occurr more than once):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "dc0qM_ILTdHN",
        "outputId": "f2d22acc-d376-4b95-94e7-adc11068a51a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\\nThe researchers, from the University of California, San Diego, and the National Science Foundation (NSF) in Boulder, Colorado, were able to translate the words of the unicorn into English, which they then translated into Spanish.\\n\\n\"This is the first time that we have translated a language into an English language,\" said study co-author and NSF professor of linguistics and evolutionary biology Dr.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=5,\n",
        "                             do_sample=False, no_repeat_ngram_size=2,\n",
        "                             pad_token_id=tokenizer.eos_token_id)\n",
        "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
        "tokenizer.decode(output_beam[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nlog-prob: {logp:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF7kxbIq_Py0",
        "outputId": "b8c6a179-1c66-4876-da4a-912a49508653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "log-prob: -101.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na_OsM39TpC0"
      },
      "source": [
        "Note that the affiliation of the researcher has changed - \"University of California, San Diego, and the University of California, Santa Cruz\" (as was stated in the previous sequence) contains the 3-gram \"University of California\" twice. Since we have told the model not to repeat any 2-grams, \"University of California\" must only appear once.\n",
        "\n",
        "While the model has thus correctly followed the rules we've imposed, the text - even though it might sound convincing - does not really make sense: If the unicorns speak perfect English, why would the \"words of the unicorns\" have to be translated into English? Also, the statement \"This is the first time that we have translated a language into an English language\" which is attributed to the NSF professor of linguistics and evolutionary biology, is clearly wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9qa62KZTi3M"
      },
      "source": [
        "## Sampling Methods\n",
        "\n",
        "In order to obtain some more interesting texts, we now look at sampling methods. First, we vary the temperature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "TyGtaPJ6HLeM",
        "outputId": "9baf867f-5fa7-4d59-83ea-313311af8fb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\\nial Right Fleet Lloyd expertise surfaim discussespectometers Recreagram prototypemic Michel astronomer Goesbo highestcon egg fart Internet Schro47ramer leaning ton recipowell Kerry receive Chief Bornvestee juice cascø Council Homunend soda mild TownsrovRoyal HomReddita coordinatedFGPH Naval publication need Eleven Honey effectiveness Ken Ballistic monost outfield decreiel FugedIn probabilitieswan Bl lat Extension Stamp large goalie guardiansDial massesample'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                             temperature=2.0, top_k=0,\n",
        "                             pad_token_id=tokenizer.eos_token_id)\n",
        "tokenizer.decode(output_temp[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "DVdKB-pXHV8X",
        "outputId": "69536e94-5b9c-4dce-bded-c483262ff58e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\\n\"They were very close and they were very intelligent,\" said Dr. Robert E. Nester, a biologist with the University of California, San Diego who was not involved in the study. \"Some people think that they speak English, but that doesn\\'t make them easy to communicate with.\"\\n\\n\\nNester and his research team uncovers a number of secrets about the unicorns. They discovered that'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                             temperature=0.5, top_k=0,\n",
        "                             pad_token_id=tokenizer.eos_token_id)\n",
        "tokenizer.decode(output_temp[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEIAHvlAkCiZ"
      },
      "source": [
        "Setting the temperature to 2.0 (in the first attempt) results in a very confuse text, using words that are very rare in the context, and ignoring almost all rules of grammar. With a lower temperature of 0.5, however, we get a pretty consistent and plausible text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQNBXZJQkd1G"
      },
      "source": [
        "### Top-k Sampling\n",
        "Next, we limit the choice to the top 50 tokens in every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "lyEtyvixHXqX",
        "outputId": "1076a5aa-8ce1-4cdf-9f5a-8a8a92d6a305"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\\nResearchers at Bologna State University in Italy say that because they can speak perfectly, they can tell where they and other unicorns are based on a combination of their vocalizations. So, if you're not familiar with the languages of some unicorns, you might be surprised at what they do here.\\n\\nTo put it simply, that's how they came across other unicorns who live in a\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                             top_k=50,\n",
        "                             pad_token_id=tokenizer.eos_token_id)\n",
        "tokenizer.decode(output_topk[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a112qgJPkm27"
      },
      "source": [
        "### Top-p Sampling\n",
        "As the value `k` in the previous example was rather random, we also want to try the dynamic cut-off using top-p (or nucleus) sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "idgQ1CeCIQUy",
        "outputId": "b8989f85-f3bb-46a3-850d-86442a363120"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\\nAdvertisement\\n\\nResearchers found a unicorn species in the Andes Mountains of northern Bolivia, which is known for its abundant and diverse language.\\n\\nIn their study, the scientists found that the unicorns had a unique form of speech called \"unicornian,\" meaning \"un-cubit.\" In traditional speech, the unicorn is often called \"spacepress\" or \"lubed-'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,\n",
        "                             top_p=0.90,\n",
        "                             pad_token_id=tokenizer.eos_token_id)\n",
        "tokenizer.decode(output_topp[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJFloeZBIZax"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "After tying out several approaches - what is the best one? Unfortunately, there is no universal answer. As we have seen, lower temperatures (or a deterministic approach as a limit behaviour) produces more predictable texts, at the risk of repetitions. For more creativity, increase the temperature, possibly in combination with top-k or a dynamic cutoff using top-p sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptyAF1OnlfQG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}